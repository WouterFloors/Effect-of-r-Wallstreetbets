# -*- coding: utf-8 -*-
"""Reddit API.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QWAPsHXooRiTttLr5KA3PcHSPRmcGp0e
"""

import csv 
import time
from datetime import date
import requests


def write_headers():
  count_headers = 0
  if count_headers == 0:
    with open("reddit_api_post_data.csv", "a") as csv_file:
            writer = csv.writer(csv_file, delimiter = "\t")
            writer.writerow(["title", "body", "permalink", "user", "upvotes", "upvote_ratio", "image", "total_awards", "flair", "num_comments", "date", "unix"])
            csv_file.close()
  count_headers = count_headers + 1

def clean(text):
  return(text.replace('\n','').replace('\t', ''))

def get_data():
  after = None
  count_data = 0
  #Get data from API  
  while (after != None) or (count_data == 0):
      url = 'https://www.reddit.com/r/wallstreetbets/top.json'
      headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=0', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}
      print('processing ' + url + ' with after parameter: ' + str(after))
      response = requests.get(url, 
                              headers=headers, 
                              params={"after": after})
      json_response = response.json()
      after = json_response['data']['after'] 

  #Specify which data to get  
      f = open("reddit_api_post_data.csv", "a")
      datetoday = str(date.today())
      timenow = str(time.time())   
      for item in json_response['data']['children']:
          try: 
            titles = item['data']['title']
          except:
            titles = ""
          try:
            bodys = item['data']['selftext']
          except:
            bodys = ""
          try:
            permalinks = item['data']['permalink']
          except:
            permalinks = ""
          try:
            users = item['data']['author']
          except:
            users = ""
          try:
            upvotes = str(item['data']['ups'])
          except:
            upvotes = ""
          try:
            upvote_ratios = str(item['data']['upvote_ratio'])
          except:
            upvote_ratios = ""
          try: 
            images = item['data']['url']
          except:
            images = ""
          try:
            total_awards = str(item['data']['total_awards_received'])
          except:
            total_awards = ""
          try:
            flairs = str(item['data']['link_flair_css_class'])
          except:
            flairs = ""
          try:
            num_comments = str(item['data']['num_comments'])
          except:
            num_comments = ""
          sep = '\t'
          f.write(clean(titles) + sep)
          f.write(clean(bodys) + sep)
          f.write(permalinks + sep)
          f.write(clean(users) + sep)
          f.write(upvotes + sep)
          f.write(upvote_ratios + sep)
          f.write(images + sep)
          f.write(total_awards + sep)
          f.write(flairs + sep)
          f.write(num_comments + sep)
          f.write(datetoday + sep)
          f.write(timenow + sep)
          f.write('\n')
      f.close()
      count_data = count_data + 1
print('done!')

write_headers()
print('headers written!')

get_data()
print('data written!')